{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the dataset, including the following files:\n",
    "\n",
    "    ```\n",
    "    dataset/\n",
    "    ‚îú‚îÄ‚îÄ data_identification.csv\n",
    "    ‚îú‚îÄ‚îÄ emotion.csv\n",
    "    ‚îú‚îÄ‚îÄ sampleSubmission.csv\n",
    "    ‚îî‚îÄ‚îÄ tweets_DM.json\n",
    "    ```\n",
    "\n",
    "    Descriptions of each file:\n",
    "\n",
    "    - `data_identification.csv:` Assign each \"tweet_id\" to a train or test label.\n",
    "\n",
    "    - `emotion.csv:` Assign each \"tweet_id\" to a emotion label.\n",
    "\n",
    "    - `sampleSubmission.csv:` Demonstration of format of submission.csv.\n",
    "\n",
    "    - `tweets_DM.json:` Primary dataset, containing tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c dm-2024-isa-5810-lab-2-homework -p dataset/\n",
    "!unzip dataset/dm-2024-isa-5810-lab-2-homework.zip -d dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load `tweets_DM.json` into a dictionary, then take out the portion of `\"tweet\"` into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                   [bibleverse]  0x28b412   \n",
       "3                             []  0x1cd5b0   \n",
       "4                             []  0x2de201   \n",
       "\n",
       "                                                text  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  \n",
       "2  Confident of your obedience, I write to you, k...  \n",
       "3                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>  \n",
       "4  \"Trust is not the same as faith. A friend is s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# This is raw data, we need to extract the train data and test data\n",
    "with open(\"dataset/tweets_DM.json\") as f:\n",
    "    tweets = [json.loads(data) for data in f]\n",
    "    tweets = [tweet[\"_source\"][\"tweet\"] for tweet in tweets]\n",
    "\n",
    "df_tweets = pd.DataFrame(tweets)\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. According to `data_identification.csv` to distinguish which \"tweet_id\" belongs to train or test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "train_id = set()\n",
    "test_id = set()\n",
    "with open(\"dataset/data_identification.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row[1] == \"train\":\n",
    "            train_id.add(row[0])\n",
    "        else:\n",
    "            test_id.add(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. According to `emotion.csv` to distinguish which \"tweet_id\" belongs to which emotion label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "emotion = {}\n",
    "with open(\"dataset/emotion.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        emotion[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Obtain `train_df` and `test_df`, and then encode the label by `LabelEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Extract the train data and test data\n",
    "train_df = df_tweets[df_tweets[\"tweet_id\"].isin(train_id)].reset_index(drop=True)\n",
    "test_df = df_tweets[df_tweets[\"tweet_id\"].isin(test_id)].reset_index(drop=True)\n",
    "# Add the emotion column to the train data\n",
    "train_df.loc[:, \"emotion\"] = train_df.apply(lambda x: emotion[x[\"tweet_id\"]], axis=1)\n",
    "train_df[\"label\"] = labelencoder.fit_transform(train_df[\"emotion\"])\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "train_df[\"text\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÈáãÊîæË®òÊÜ∂È´î\n",
    "del df_tweets\n",
    "del tweets\n",
    "del emotion\n",
    "del train_id\n",
    "del test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, \"length\"] = train_df[\"text\"].apply(lambda x: len(x.split()))\n",
    "train_df.loc[:, \"<LH>\"] = train_df[\"text\"].apply(lambda x: x.count(\"<LH>\"))\n",
    "train_df.loc[:, \"@\"] = train_df[\"text\"].apply(lambda x: x.count(\"@\"))\n",
    "train_df.loc[:, \"#\"] = train_df[\"text\"].apply(lambda x: x.count(\"#\"))\n",
    "train_df.loc[:, \"trash rate\"] = (\n",
    "    train_df[\"<LH>\"] + train_df[\"@\"] + train_df[\"#\"]\n",
    ") / train_df[\"length\"]\n",
    "\n",
    "test_df.loc[:, \"length\"] = test_df[\"text\"].apply(lambda x: len(x.split()))\n",
    "test_df.loc[:, \"<LH>\"] = test_df[\"text\"].apply(lambda x: x.count(\"<LH>\"))\n",
    "test_df.loc[:, \"@\"] = test_df[\"text\"].apply(lambda x: x.count(\"@\"))\n",
    "test_df.loc[:, \"#\"] = test_df[\"text\"].apply(lambda x: x.count(\"#\"))\n",
    "test_df.loc[:, \"trash rate\"] = (\n",
    "    test_df[\"<LH>\"] + test_df[\"@\"] + test_df[\"#\"]\n",
    ") / test_df[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with (trash rate >= 0.67): 38505\n",
      "Number of rows with (<LH> > 4): 39817\n"
     ]
    }
   ],
   "source": [
    "trash_rate = 2 / 3\n",
    "lh_max = 4\n",
    "\n",
    "lh_rate_count = train_df[train_df[\"trash rate\"] >= trash_rate].shape[0]\n",
    "lh_count = train_df[train_df[\"<LH>\"] > lh_max].shape[0]\n",
    "\n",
    "print(f\"Number of rows with (trash rate >= {trash_rate:.2f}): {lh_rate_count}\")\n",
    "print(f\"Number of rows with (<LH> > {lh_max}): {lh_count}\")\n",
    "\n",
    "\n",
    "# Filter the rows with <LH> rate >= 2/3 or <LH> >= 5\n",
    "filtered_df = train_df[\n",
    "    (train_df[\"trash rate\"] >= trash_rate) | (train_df[\"<LH>\"] > lh_max)\n",
    "]\n",
    "\n",
    "# Convert the filtered dataframe to a list of dictionaries\n",
    "filtered_texts = filtered_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Save the list of dictionaries to a JSON file\n",
    "with open(\"filtered_texts.json\", \"w\") as outfile:\n",
    "    json.dump(filtered_texts, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with (trash rate >= 0.67): 548\n",
      "Number of rows with (<LH> > 4): 0\n"
     ]
    }
   ],
   "source": [
    "trash_rate = 2 / 3\n",
    "lh_max = 4\n",
    "\n",
    "lh_rate_count = test_df[test_df[\"trash rate\"] >= trash_rate].shape[0]\n",
    "lh_count = test_df[test_df[\"<LH>\"] > lh_max].shape[0]\n",
    "\n",
    "print(f\"Number of rows with (trash rate >= {trash_rate:.2f}): {lh_rate_count}\")\n",
    "print(f\"Number of rows with (<LH> > {lh_max}): {lh_count}\")\n",
    "\n",
    "\n",
    "# Filter the rows with <LH> rate >= 2/3 or <LH> >= 5\n",
    "filtered_df = test_df[\n",
    "    (test_df[\"trash rate\"] >= trash_rate) | (test_df[\"<LH>\"] > lh_max)\n",
    "]\n",
    "\n",
    "# Convert the filtered dataframe to a list of dictionaries\n",
    "filtered_texts = filtered_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Save the list of dictionaries to a JSON file\n",
    "with open(\"filtered_texts_test.json\", \"w\") as outfile:\n",
    "    json.dump(filtered_texts, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with trash rate >= 2/3 or <LH> >= 5\n",
    "train_df = train_df[\n",
    "    (train_df[\"trash rate\"] < trash_rate) & (train_df[\"<LH>\"] <= lh_max)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "Number of rows with (trash rate >= 0.67): 0\n",
      "Number of rows with (<LH> > 4): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"train data\")\n",
    "lh_rate_count = train_df[train_df[\"trash rate\"] >= trash_rate].shape[0]\n",
    "lh_count = train_df[train_df[\"<LH>\"] > lh_max].shape[0]\n",
    "\n",
    "print(f\"Number of rows with (trash rate >= {trash_rate:.2f}): {lh_rate_count}\")\n",
    "print(f\"Number of rows with (<LH> > {lh_max}): {lh_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>&lt;LH&gt;</th>\n",
       "      <th>@</th>\n",
       "      <th>#</th>\n",
       "      <th>trash rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x368e95</td>\n",
       "      <td>Love knows no gender. üò¢üò≠ &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[LeagueCup]</td>\n",
       "      <td>0x249c0c</td>\n",
       "      <td>@DStvNgCare @DStvNg More highlights are being ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[SSM, gender, diversity]</td>\n",
       "      <td>0x359db9</td>\n",
       "      <td>The #SSM debate; &lt;LH&gt; (a manufactured fantasy ...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x23b037</td>\n",
       "      <td>I love suffering üôÉüôÉ I love when valium does no...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Pissed]</td>\n",
       "      <td>0x1fde89</td>\n",
       "      <td>Can someone tell my why my feeds scroll back t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                             []  0x1cd5b0   \n",
       "3      [authentic, LaughOutLoud]  0x1d755c   \n",
       "4                             []  0x2c91a8   \n",
       "5                             []  0x368e95   \n",
       "6                    [LeagueCup]  0x249c0c   \n",
       "7       [SSM, gender, diversity]  0x359db9   \n",
       "8                             []  0x23b037   \n",
       "9                       [Pissed]  0x1fde89   \n",
       "\n",
       "                                                text       emotion  label  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  anticipation      1   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...       sadness      5   \n",
       "2                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>          fear      3   \n",
       "3  @RISKshow @TheKevinAllison Thx for the BEST TI...           joy      4   \n",
       "4       Still waiting on those supplies Liscus. <LH>  anticipation      1   \n",
       "5                      Love knows no gender. üò¢üò≠ <LH>           joy      4   \n",
       "6  @DStvNgCare @DStvNg More highlights are being ...       sadness      5   \n",
       "7  The #SSM debate; <LH> (a manufactured fantasy ...  anticipation      1   \n",
       "8  I love suffering üôÉüôÉ I love when valium does no...           joy      4   \n",
       "9  Can someone tell my why my feeds scroll back t...         anger      0   \n",
       "\n",
       "   length  <LH>  @  #  trash rate  \n",
       "0      14     1  0  1    0.142857  \n",
       "1      18     2  1  3    0.333333  \n",
       "2       7     1  0  0    0.142857  \n",
       "3      15     1  2  2    0.333333  \n",
       "4       7     1  0  0    0.142857  \n",
       "5       6     1  0  0    0.166667  \n",
       "6      17     1  2  1    0.235294  \n",
       "7      22     1  0  3    0.181818  \n",
       "8      27     1  0  0    0.037037  \n",
       "9      21     0  0  1    0.047619  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>&lt;LH&gt;</th>\n",
       "      <th>@</th>\n",
       "      <th>#</th>\n",
       "      <th>trash rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x31c6e0</td>\n",
       "      <td>Turns out you can recognise people by their un...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[sheltered]</td>\n",
       "      <td>0x32edee</td>\n",
       "      <td>I like how Hayvens mommy, daddy, and the keybo...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[notamused]</td>\n",
       "      <td>0x3714ee</td>\n",
       "      <td>I just love it when every single one of my son...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[CelebrityBigBrother]</td>\n",
       "      <td>0x235628</td>\n",
       "      <td>@JulieChen when can we expect a season of #Cel...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x283024</td>\n",
       "      <td>Tbh. Regret hurts more than stepping on a LEGO...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hashtags  tweet_id  \\\n",
       "0                       [bibleverse]  0x28b412   \n",
       "1                                 []  0x2de201   \n",
       "2  [materialism, money, possessions]  0x218443   \n",
       "3               [GodsPlan, GodsWork]  0x2939d5   \n",
       "4                                 []  0x26289a   \n",
       "5                                 []  0x31c6e0   \n",
       "6                        [sheltered]  0x32edee   \n",
       "7                        [notamused]  0x3714ee   \n",
       "8              [CelebrityBigBrother]  0x235628   \n",
       "9                                 []  0x283024   \n",
       "\n",
       "                                                text  length  <LH>  @  #  \\\n",
       "0  Confident of your obedience, I write to you, k...      24     2  0  1   \n",
       "1  \"Trust is not the same as faith. A friend is s...      25     2  0  0   \n",
       "2  When do you have enough ? When are you satisfi...      23     1  0  3   \n",
       "3  God woke you up, now chase the day #GodsPlan #...      11     1  0  2   \n",
       "4  In these tough times, who do YOU turn to as yo...      15     1  0  0   \n",
       "5  Turns out you can recognise people by their un...      10     1  0  0   \n",
       "6  I like how Hayvens mommy, daddy, and the keybo...      22     1  0  1   \n",
       "7  I just love it when every single one of my son...      24     1  0  1   \n",
       "8  @JulieChen when can we expect a season of #Cel...      15     1  1  1   \n",
       "9  Tbh. Regret hurts more than stepping on a LEGO...      10     1  0  0   \n",
       "\n",
       "   trash rate  \n",
       "0    0.125000  \n",
       "1    0.080000  \n",
       "2    0.173913  \n",
       "3    0.272727  \n",
       "4    0.066667  \n",
       "5    0.100000  \n",
       "6    0.090909  \n",
       "7    0.083333  \n",
       "8    0.200000  \n",
       "9    0.100000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions  # Ê™¢Êü•ÊòØÂê¶ÊúâÁ∏ÆÂØ´\n",
    "import emoji\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    # text = contractions.fix(text)  # expand contractions\n",
    "    # text = re.sub(r\"http\\S+\", \"[URL]\", text)  # remove URL\n",
    "    text = re.sub(r\"(https?://)?[\\w.-]+\\.com(\\.\\w+)?\", \"<URL>\", text)\n",
    "    # text = re.sub(r\"@\\S+\", \"\", text)  # remove mentions\n",
    "    # text = re.sub(r\"#\\S+\", \"\", text)  # remove hashtag\n",
    "    # text = re.sub(r\"\\+\\S+\", \"\", text)  # remove phone number\n",
    "    text = re.sub(r\"<LH>\", \"<mask>\", text)  # remove <LH>\n",
    "    # text = emoji.replace_emoji(text, replace=\"\")  # remove emojis\n",
    "    # text = re.sub(r\"(\\W)\\1+\", r\"\\1\", text)  # remove repeating characters\n",
    "    # text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuations\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove extra whitespaces\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/S113062615/DM-2024/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't miss this amazing offer! Check it out now: <URL> #AmazingOffer üòäüòäüòä @username123 <mask> Save $100 today! üéâüéâüéâ #DealOfTheDay Contact us at +123456789. <mask>\n",
      "['‚ñÅDon', \"'\", 't', '‚ñÅmiss', '‚ñÅthis', '‚ñÅamazing', '‚ñÅoffer', '!', '‚ñÅCheck', '‚ñÅit', '‚ñÅout', '‚ñÅnow', ':', '‚ñÅ<', 'URL', '>', '‚ñÅ#', 'A', 'maz', 'ing', 'Off', 'er', '‚ñÅ', 'üòä', 'üòä', 'üòä', '‚ñÅ@', 'user', 'name', '123', ' <mask>', '‚ñÅSave', '‚ñÅ$100', '‚ñÅtoday', '!', '‚ñÅ', 'üéâ', 'üéâ', 'üéâ', '‚ñÅ#', 'De', 'al', 'Of', 'The', 'Day', '‚ñÅContact', '‚ñÅus', '‚ñÅat', '‚ñÅ+', '1234', '56', '789', '.', ' <mask>']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Twitter/twhin-bert-base\")\n",
    "\n",
    "text = \"Don't miss this amazing offer! Check it out now: http://example.com #AmazingOffer üòäüòäüòä @username123 <LH> Save $100 today! üéâüéâüéâ #DealOfTheDay Contact us at +123456789. <LH>\"\n",
    "\n",
    "text = preprocess_tweet(text)\n",
    "print(text)\n",
    "token_text = tokenizer.tokenize(text)\n",
    "print(token_text)\n",
    "print(type(token_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1375950/1375950 [00:23<00:00, 59486.74it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 411972/411972 [00:07<00:00, 57964.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def preprocess_with_progress(data, func, num_workers=4):\n",
    "    with Pool(num_workers) as pool:\n",
    "        # ‰ΩøÁî® tqdm ÂåÖË£ùÈÄ≤Â∫¶Ê¢ù\n",
    "        results = list(tqdm(pool.imap(func, data), total=len(data)))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Â∞çË®ìÁ∑¥ÂíåÊ∏¨Ë©¶Êï∏ÊìöÈõÜÊáâÁî®\n",
    "train_df[\"text\"] = preprocess_with_progress(train_df[\"text\"], preprocess_tweet)\n",
    "test_df[\"text\"] = preprocess_with_progress(test_df[\"text\"], preprocess_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data in this training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>&lt;LH&gt;</th>\n",
       "      <th>@</th>\n",
       "      <th>#</th>\n",
       "      <th>trash rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;mask&gt;</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;mask&gt;</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x368e95</td>\n",
       "      <td>Love knows no gender. üò¢üò≠ &lt;mask&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[LeagueCup]</td>\n",
       "      <td>0x249c0c</td>\n",
       "      <td>@DStvNgCare @DStvNg More highlights are being ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[SSM, gender, diversity]</td>\n",
       "      <td>0x359db9</td>\n",
       "      <td>The #SSM debate; &lt;mask&gt; (a manufactured fantas...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x23b037</td>\n",
       "      <td>I love suffering üôÉüôÉ I love when valium does no...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Pissed]</td>\n",
       "      <td>0x1fde89</td>\n",
       "      <td>Can someone tell my why my feeds scroll back t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                             []  0x1cd5b0   \n",
       "3      [authentic, LaughOutLoud]  0x1d755c   \n",
       "4                             []  0x2c91a8   \n",
       "5                             []  0x368e95   \n",
       "6                    [LeagueCup]  0x249c0c   \n",
       "7       [SSM, gender, diversity]  0x359db9   \n",
       "8                             []  0x23b037   \n",
       "9                       [Pissed]  0x1fde89   \n",
       "\n",
       "                                                text       emotion  label  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  anticipation      1   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...       sadness      5   \n",
       "2              Now ISSA is stalking Tasha üòÇüòÇüòÇ <mask>          fear      3   \n",
       "3  @RISKshow @TheKevinAllison Thx for the BEST TI...           joy      4   \n",
       "4     Still waiting on those supplies Liscus. <mask>  anticipation      1   \n",
       "5                    Love knows no gender. üò¢üò≠ <mask>           joy      4   \n",
       "6  @DStvNgCare @DStvNg More highlights are being ...       sadness      5   \n",
       "7  The #SSM debate; <mask> (a manufactured fantas...  anticipation      1   \n",
       "8  I love suffering üôÉüôÉ I love when valium does no...           joy      4   \n",
       "9  Can someone tell my why my feeds scroll back t...         anger      0   \n",
       "\n",
       "   length  <LH>  @  #  trash rate  \n",
       "0      14     1  0  1    0.142857  \n",
       "1      18     2  1  3    0.333333  \n",
       "2       7     1  0  0    0.142857  \n",
       "3      15     1  2  2    0.333333  \n",
       "4       7     1  0  0    0.142857  \n",
       "5       6     1  0  0    0.166667  \n",
       "6      17     1  2  1    0.235294  \n",
       "7      22     1  0  3    0.181818  \n",
       "8      27     1  0  0    0.037037  \n",
       "9      21     0  0  1    0.047619  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>&lt;LH&gt;</th>\n",
       "      <th>@</th>\n",
       "      <th>#</th>\n",
       "      <th>trash rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x31c6e0</td>\n",
       "      <td>Turns out you can recognise people by their un...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[sheltered]</td>\n",
       "      <td>0x32edee</td>\n",
       "      <td>I like how Hayvens mommy, daddy, and the keybo...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[notamused]</td>\n",
       "      <td>0x3714ee</td>\n",
       "      <td>I just love it when every single one of my son...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[CelebrityBigBrother]</td>\n",
       "      <td>0x235628</td>\n",
       "      <td>@JulieChen when can we expect a season of #Cel...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x283024</td>\n",
       "      <td>Tbh. Regret hurts more than stepping on a LEGO...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hashtags  tweet_id  \\\n",
       "0                       [bibleverse]  0x28b412   \n",
       "1                                 []  0x2de201   \n",
       "2  [materialism, money, possessions]  0x218443   \n",
       "3               [GodsPlan, GodsWork]  0x2939d5   \n",
       "4                                 []  0x26289a   \n",
       "5                                 []  0x31c6e0   \n",
       "6                        [sheltered]  0x32edee   \n",
       "7                        [notamused]  0x3714ee   \n",
       "8              [CelebrityBigBrother]  0x235628   \n",
       "9                                 []  0x283024   \n",
       "\n",
       "                                                text  length  <LH>  @  #  \\\n",
       "0  Confident of your obedience, I write to you, k...      24     2  0  1   \n",
       "1  \"Trust is not the same as faith. A friend is s...      25     2  0  0   \n",
       "2  When do you have enough ? When are you satisfi...      23     1  0  3   \n",
       "3  God woke you up, now chase the day #GodsPlan #...      11     1  0  2   \n",
       "4  In these tough times, who do YOU turn to as yo...      15     1  0  0   \n",
       "5  Turns out you can recognise people by their un...      10     1  0  0   \n",
       "6  I like how Hayvens mommy, daddy, and the keybo...      22     1  0  1   \n",
       "7  I just love it when every single one of my son...      24     1  0  1   \n",
       "8  @JulieChen when can we expect a season of #Cel...      15     1  1  1   \n",
       "9  Tbh. Regret hurts more than stepping on a LEGO...      10     1  0  0   \n",
       "\n",
       "   trash rate  \n",
       "0    0.125000  \n",
       "1    0.080000  \n",
       "2    0.173913  \n",
       "3    0.272727  \n",
       "4    0.066667  \n",
       "5    0.100000  \n",
       "6    0.090909  \n",
       "7    0.083333  \n",
       "8    0.200000  \n",
       "9    0.100000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"Twitter/twhin-bert-base\"\n",
    "\n",
    "# Hyperparameters\n",
    "# gamma = 0.95\n",
    "train_batch_size = 256\n",
    "val_batch_size = 256\n",
    "dropout_rate = 0.1\n",
    "lr = 2e-5\n",
    "epochs = 8\n",
    "val_split = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.id = df[\"tweet_id\"].tolist()\n",
    "        self.text = df[\"text\"].tolist()\n",
    "        if \"label\" in df.columns:\n",
    "            self.label = df[\"label\"].tolist()\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"id\": self.id[idx], \"text\": self.text[idx]}\n",
    "        if self.label is not None:\n",
    "            item[\"label\"] = self.label[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "# Split the train data into training and validation sets\n",
    "train_data, val_data = train_test_split(train_df, test_size=val_split, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "ds_train = TweetDataset(train_data)\n",
    "ds_val = TweetDataset(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Load the dataset and finish the preprocessing\n",
    "def collate_fn(batch):\n",
    "    inputs = tokenizer(\n",
    "        [data[\"text\"] for data in batch],\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    labels = torch.tensor([data[\"label\"] for data in batch], dtype=torch.long)\n",
    "    labels = labels.to(device)\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at Twitter/twhin-bert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class TweetEmotionClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, 8)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = self.bert(**kwargs)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.linear(cls_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def extract_features(self, **kwargs):\n",
    "        output = self.bert(**kwargs)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]\n",
    "        return cls_output\n",
    "\n",
    "\n",
    "model = TweetEmotionClassifier(model_name, dropout=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2930, 0.0497, 0.0844, 0.1835, 0.0242, 0.0609, 0.2464, 0.0580])\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "class_counts = train_df[\"label\"].value_counts()\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights /= class_weights.sum()\n",
    "class_weights = torch.tensor(class_weights.sort_index().values, dtype=torch.float32)\n",
    "print(class_weights)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "# scheduler = ExponentialLR(\n",
    "#     optimizer,\n",
    "#     # gamma=gamma,\n",
    "# )\n",
    "\n",
    "criteria = torch.nn.CrossEntropyLoss()  # weight=class_weights).to(device)\n",
    "\n",
    "acc = MulticlassAccuracy(num_classes=class_counts).to(device)\n",
    "f1 = MulticlassF1Score(num_classes=class_counts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# # Warmup\n",
    "# total_steps = len(dl_train) * epochs\n",
    "\n",
    "# warmup_ratio = 0.1\n",
    "# warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=warmup_steps,\n",
    "#     num_training_steps=total_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/8]:   0%|          | 0/4838 [00:00<?, ?it/s]BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Training Epoch [1/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4838/4838 [45:02<00:00,  1.79it/s, loss=1.04, lr=2e-5]\n",
      "Validation Epoch [1/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 538/538 [01:48<00:00,  4.95it/s, loss=0.936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6613\n",
      "F1 Score: 0.6613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [2/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4838/4838 [44:59<00:00,  1.79it/s, loss=0.896, lr=2e-5]\n",
      "Validation Epoch [2/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 538/538 [01:47<00:00,  4.99it/s, loss=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6690\n",
      "F1 Score: 0.6690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4838/4838 [44:58<00:00,  1.79it/s, loss=0.824, lr=2e-5]\n",
      "Validation Epoch [3/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 538/538 [01:47<00:00,  5.02it/s, loss=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6771\n",
      "F1 Score: 0.6771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [4/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4838/4838 [45:03<00:00,  1.79it/s, loss=0.761, lr=2e-5] \n",
      "Validation Epoch [4/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 538/538 [01:47<00:00,  5.00it/s, loss=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6784\n",
      "F1 Score: 0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [5/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4838/4838 [45:02<00:00,  1.79it/s, loss=0.7, lr=2e-5]  \n",
      "Validation Epoch [5/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 538/538 [01:47<00:00,  5.01it/s, loss=0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6768\n",
      "F1 Score: 0.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [6/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4838/4838 [45:00<00:00,  1.79it/s, loss=0.644, lr=2e-5]\n",
      "Validation Epoch [6/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 538/538 [01:47<00:00,  5.00it/s, loss=0.978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6746\n",
      "F1 Score: 0.6746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [7/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4838/4838 [45:01<00:00,  1.79it/s, loss=0.588, lr=2e-5]\n",
      "Validation Epoch [7/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 538/538 [01:47<00:00,  5.00it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6707\n",
      "F1 Score: 0.6707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [8/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4838/4838 [44:59<00:00,  1.79it/s, loss=0.536, lr=2e-5]\n",
      "Validation Epoch [8/8]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 538/538 [01:48<00:00,  4.96it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6663\n",
      "F1 Score: 0.6663\n",
      "Best model is at epoch 2 with loss 487.21711856126785\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "saved_dic = f\"./twhin-bert_b{train_batch_size}_Wpre_v1_&_rm_LH\"\n",
    "if not os.path.exists(saved_dic):\n",
    "    os.makedirs(saved_dic)\n",
    "\n",
    "best_model = {\"ep\": -1, \"loss\": float(\"inf\")}\n",
    "model.to(device)\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    bar = tqdm(dl_train, desc=f\"Training Epoch [{ep + 1}/{epochs}]\")\n",
    "    train_loss = 0\n",
    "    for inputs, labels in bar:\n",
    "        # inputs = inputs.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(**inputs)\n",
    "        loss = criteria(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        bar.set_postfix(\n",
    "            loss=train_loss / (bar.n + 1), lr=optimizer.param_groups[0][\"lr\"]\n",
    "        )\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    bar = tqdm(dl_val, desc=f\"Validation Epoch [{ep + 1}/{epochs}]\")\n",
    "    val_loss = 0\n",
    "    acc.reset()\n",
    "    f1.reset()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in bar:\n",
    "            # inputs = inputs.to(device)\n",
    "            # labels = labels.to(device)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "            loss = criteria(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            bar.set_postfix(loss=val_loss / (bar.n + 1))\n",
    "\n",
    "            acc.update(logits, labels)\n",
    "            f1.update(logits, labels)\n",
    "\n",
    "    if val_loss < best_model[\"loss\"]:\n",
    "        best_model[\"ep\"] = ep\n",
    "        best_model[\"loss\"] = val_loss\n",
    "\n",
    "    print(f\"Accuracy: {acc.compute():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute():.4f}\")\n",
    "    torch.save(model, f\"{saved_dic}/ep{ep}.ckpt\")\n",
    "\n",
    "print(f\"Best model is at epoch {best_model['ep']} with loss {best_model['loss']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
